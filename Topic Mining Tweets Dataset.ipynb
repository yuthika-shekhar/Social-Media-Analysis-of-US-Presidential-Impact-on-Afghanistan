{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    \n",
    "### Data Mining Project \n",
    "\n",
    "### Project: US Presidential Impact on Afghanistan\n",
    "### Team Members: Aditya Taori and Yuthika Shekhar\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd  \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import Phrases\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "import pickle \n",
    "import pyLDAvis.gensim_models\n",
    "import os\n",
    "from nltk.tokenize import RegexpTokenizer,sent_tokenize,word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting Up Input Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_dir = \"E:/UOR Notes/Data Mining/Assignments/US Presidential Impact/Output_Files/Tweets Data/\"\n",
    "input_file = \"Osama bin Laden Killed.csv\"\n",
    "input_path = input_dir+input_file\n",
    "event_name = \"Osama Bin Laden Killed\"\n",
    "output_dir = \"E:/UOR Notes/Data Mining/Assignments/US Presidential Impact/Output_Files/Topic Modelling/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading Tweets Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv(input_path)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_df.shape)\n",
    "tweets_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out duplicate tweet ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_event_data = tweets_df.drop_duplicates(subset=[\"id_str\"])\n",
    "print(unique_event_data.shape)\n",
    "unique_event_data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Retweets from Tweets Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = unique_event_data[\"text\"].str.contains('^RT',regex=True)\n",
    "y.value_counts()\n",
    "retweets = unique_event_data[y]\n",
    "indexes = retweets.index\n",
    "indexes.tolist()\n",
    "print(unique_event_data.shape)\n",
    "unique_event_data = unique_event_data.drop(indexes)\n",
    "print(unique_event_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Hyperlinks from Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_wo_hyperlinks = unique_event_data[\"text\"].str.lower().replace('http\\S+',\"\",regex=True)\n",
    "#create_wordcloud(tweets_wo_hyperlinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Hashtags from Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_wo_hashtags = tweets_wo_hyperlinks.replace(\"#[A-Za-z0-9]*\",\"\",regex=True)\n",
    "#create_wordcloud(tweets_wo_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Tags from Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_wo_tags = tweets_wo_hashtags.replace(\"\\@[A-Za-z0-9]*\",\"\",regex=True)\n",
    "#create_wordcloud(tweets_wo_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords from Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedStopWords = stopwords.words(\"english\")\n",
    "regexes= r'\\b(' + r'|'.join(cachedStopWords) + r')\\b\\s*'\n",
    "tweets_wo_sw= tweets_wo_tags.replace(regexes,\"\",regex=True)\n",
    "#create_wordcloud(tweets_wo_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_event_data[\"Cleaned_Data\"] = tweets_wo_sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Empty Rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_event_data['Cleaned_Data'].dropna(inplace=True)\n",
    "print(unique_event_data.shape)\n",
    "docs = np.array(unique_event_data['Cleaned_Data'])\n",
    "docs = [doc for doc in docs if str(doc) != 'nan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_preprocessor(docs):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    print(len(docs))\n",
    "    for idx in range(len(docs)):\n",
    "        #print(idx)\n",
    "        #print(docs[idx])\n",
    "                #docs[idx] = [[word.lower() for word in docs[idx].split()] for line in data] # Convert to lowercase.\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "        #print(docs[idx])\n",
    "            # Remove numbers, but not words that contain numbers.\n",
    "    docs_token = [[token.lower() for token in doc if not token.isdigit()] for doc in docs]\n",
    "\n",
    "            # Remove words that are only one character.\n",
    "    docs = [[token for token in doc if len(token) > 3] for doc in docs]\n",
    "\n",
    "            # Lemmatize all words in documents.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = docs_preprocessor(docs)\n",
    "#docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phrases(docs, min_count=10)\n",
    "bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = Phrases(bigram[docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Bigrams and Trigrams to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(docs)\n",
    "\n",
    "for idx in range(len(docs)):\n",
    "    #print(idx)\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "                    # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)\n",
    "            for token in trigram[docs[idx]]:\n",
    "                if '_' in token:\n",
    "                    # Token is a bigram, add to document.\n",
    "                    docs[idx].append(token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "print('Number of unique words in initital documents:', len(dictionary))\n",
    "\n",
    "# Filter out words that occur less than 10 documents, or more than 20% of the documents.\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.2)\n",
    "print('Number of unique words after removing rare and common words:', len(dictionary))\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Set training parameters.\n",
    "num_topics = range(5,6)\n",
    "chunksize = 100 # size of the doc looked at every pass\n",
    "passes = 20 # number of passes through documents\n",
    "iterations = 400\n",
    "eval_every = 1  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "topic_hp_df = pd.DataFrame(columns = [\"Event Name\",\"Number of Topics\",\"Coherence Score\",\"Perplexity Score\",\"Iterations\",\"Chunksize\",\"Time Taken\",\"Keywords in Topics\"])\n",
    "\n",
    "\n",
    "for num in num_topics:\n",
    "    print(num)\n",
    "    inter_topic_df = pd.DataFrame(columns = [\"Event Name\",\"Number of Topics\",\"Coherence Score\",\"Perplexity Score\",\"Iterations\",\"Chunksize\",\"Time Taken\",\"Keywords in Topics\"])\n",
    "    # Make a index to word dictionary.\n",
    "    if len(dictionary)>0:\n",
    "        temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "        start = time.time()\n",
    "        id2word = dictionary.id2token\n",
    "        model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize,alpha='auto', eta='auto', iterations=iterations, num_topics=num,passes=passes, eval_every=eval_every)\n",
    "        end = time.time()\n",
    "        print(end-start)\n",
    "        time_taken = end-start\n",
    "\n",
    "        Y = model.print_topics(num_words = 10)\n",
    "        topic_words_list = []\n",
    "        for i in range(0,len(Y)):\n",
    "            #topic_words_list.append(html.Br())\n",
    "            topic_words_list.append(\"Topic \"+str(i+1)+\": \")\n",
    "            X = Y[i][1]\n",
    "            a = X.split(\"+\")\n",
    "            words_str = \"\"\n",
    "            for j in range(0,len(a)):\n",
    "                p = a[j].split(\"*\")[1].replace(\"\\\"\",\"\")\n",
    "                words_str += p\n",
    "            topic_words_list.append(words_str)\n",
    "\n",
    "        perplexity = model.log_perplexity(corpus)\n",
    "        coherence_model_lda = CoherenceModel(model=model, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "        \n",
    "        inter_topic_df[\"Event Name\"] = [event_name]\n",
    "        inter_topic_df[\"Number of Topics\"] = [num]\n",
    "        inter_topic_df[\"Coherence Score\"] = [coherence_lda]\n",
    "        inter_topic_df[\"Perplexity Score\"] = [perplexity]\n",
    "        inter_topic_df[\"Iterations\"] = [iterations]\n",
    "        inter_topic_df[\"Chunksize\"] = [chunksize]\n",
    "        inter_topic_df[\"Time Taken\"] = [time_taken]\n",
    "        inter_topic_df[\"Keywords in Topics\"] = [topic_words_list]\n",
    "        topic_hp_df = pd.concat([topic_hp_df,inter_topic_df])\n",
    "        \n",
    "        LDAvis_data_filepath = os.path.join(output_dir + event_name +'_sample_ldavis')\n",
    "        # # this is a bit time consuming - make the if statement True\n",
    "        # # if you want to execute visualization prep yourself\n",
    "        if 1 == 1:\n",
    "            LDAvis_prepared = pyLDAvis.gensim_models.prepare(model, corpus, dictionary)\n",
    "            with open(LDAvis_data_filepath, 'wb') as f:\n",
    "                pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "        # load the pre-prepared pyLDAvis data from disk\n",
    "        with open(LDAvis_data_filepath,'rb') as f:\n",
    "            LDAvis_prepared = pickle.load(f)\n",
    "        pyLDAvis.save_html(LDAvis_prepared, output_dir + event_name +'_sample_ldavis.html')\n",
    "\n",
    "\n",
    "#topic_hp_df.to_csv(input_dir+\"Topic_Modelling_Hyperparameter_Tuning.csv\")\n",
    "#topic_hp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(LDAvis_prepared, output_dir + event_name +'_sample_ldavis.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic to Sentence Distribution Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df[\"Topic\"] = \"99\"\n",
    "\n",
    "for i in range(0,len(corpus)):\n",
    "    #sent_topic_mapping[\"Sentence\"].iloc[i] = p_df['DESCRIPTION'].iloc[i]\n",
    "    print(i)\n",
    "    topic_dist = model[corpus[i]]\n",
    "    inter_topic = []\n",
    "    topic_scores = []\n",
    "    for j in range(0,len(topic_dist)):\n",
    "        inter_topic.append(topic_dist[j][0])\n",
    "        topic_scores.append(topic_dist[j][1])\n",
    "    print(inter_topic)\n",
    "    print(topic_scores)\n",
    "    tweets_df[\"Topic\"].iloc[i]= inter_topic[topic_scores.index(max(topic_scores))]+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the data to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = output_dir + event_name + \"All_Data_Topic_Sentence_Distribution.csv\"\n",
    "tp_word_output = output_dir + event_name + \"All_Data_Topic_Word_Distribution.csv\"\n",
    "tweets_df.to_csv(filename)\n",
    "topic_hp_df.to_csv(tp_word_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
